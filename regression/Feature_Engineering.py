#Write a function, select_kbest_chisquared() that takes X_train, y_train and k as input (X_train and y_train should not be scaled!) and returns a list of the top k features.



#Write a function, select_kbest_freg() that takes X_train, y_train (scaled) and k as input and returns a list of the top k features.


#Write a function, ols_backware_elimination() that takes X_train and y_train (scaled) as input and returns selected features based on the ols backwards elimination method


#Write a function, lasso_cv_coef() that takes X_train and y_train as input and returns the coefficients for each feature, along with a plot of the features and their weights.

#Write 3 functions, the first computes the number of optimum features (n) using rfe, the second takes n as input and returns the top n features, and the third takes the list of the top n features as input and returns a new X_train and X_test dataframe with those top features , recursive_feature_elimination() that computes the optimum number of features (n) and returns the top n features.